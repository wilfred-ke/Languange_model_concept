{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "94c20b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# importing modules\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "393eb8ed",
   "metadata": {},
   "source": [
    "# PROBLEM 1 – Reading the data \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64a2eafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hide new secretions from the parental units</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>contains no wit , only labored gags</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that loves its characters and communicates som...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>remains utterly satisfied to remain the same t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>on the worst revenge-of-the-nerds clichés the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0       hide new secretions from the parental units       0\n",
       "1               contains no wit , only labored gags       0\n",
       "2  that loves its characters and communicates som...      1\n",
       "3  remains utterly satisfied to remain the same t...      0\n",
       "4  on the worst revenge-of-the-nerds clichés the ...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Step 1: Reading in the data from \"train.tsv\" using pandas\n",
    "data = pd.read_csv(\"train.tsv\", sep='\\t')\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "185fc5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** First 5 rows of the training dataset are \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hide new secretions from the parental units</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>contains no wit , only labored gags</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>that loves its characters and communicates som...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>remains utterly satisfied to remain the same t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>on the worst revenge-of-the-nerds clichés the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence  label\n",
       "0       hide new secretions from the parental units       0\n",
       "1               contains no wit , only labored gags       0\n",
       "2  that loves its characters and communicates som...      1\n",
       "3  remains utterly satisfied to remain the same t...      0\n",
       "4  on the worst revenge-of-the-nerds clichés the ...      0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 2: Spliting  the dataset into train, validation, and test sets\n",
    "# Validation set 100 rows\n",
    "validation_set = data.sample(n=100, random_state=42)\n",
    "validation_dataset=pd.DataFrame(validation_set)\n",
    "validation_dataset.to_csv('validation_dataset.csv',index=False)\n",
    "data = data.drop(validation_set.index)\n",
    "\n",
    "# Testing  set 100 rows\n",
    "test_set = data.sample(n=100, random_state=42)\n",
    "test_dataset=pd.DataFrame(test_set)\n",
    "test_dataset.to_csv(\"test_dataset.csv\",index=False)\n",
    "data = data.drop(test_set.index)\n",
    "\n",
    "# Training set is equal to the remaining rows\n",
    "training_set = data\n",
    "train_dataset=pd.DataFrame(training_set)\n",
    "train_dataset.to_csv(\"train_dataset.csv\",index=False)\n",
    "\n",
    "print(\"****** First 5 rows of the training dataset are \")\n",
    "train_dataset.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91b11113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** First 5 rows of the testing  dataset are \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38900</th>\n",
       "      <td>burnt out</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44118</th>\n",
       "      <td>it could be , by its art and heart , a necessa...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5530</th>\n",
       "      <td>dying</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39031</th>\n",
       "      <td>, an interesting and at times captivating take...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24722</th>\n",
       "      <td>clumsy and rushed</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  label\n",
       "38900                                         burnt out       0\n",
       "44118  it could be , by its art and heart , a necessa...      1\n",
       "5530                                              dying       0\n",
       "39031  , an interesting and at times captivating take...      1\n",
       "24722                                 clumsy and rushed       0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "print(\"****** First 5 rows of the testing  dataset are \\n\")\n",
    "test_dataset.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab061579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** First 5 rows of the validation   dataset are \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49752</th>\n",
       "      <td>crisp framing</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24709</th>\n",
       "      <td>dislocation</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34945</th>\n",
       "      <td>of the problems with the film</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28707</th>\n",
       "      <td>'s ) a clever thriller with enough unexpected ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3013</th>\n",
       "      <td>in perfect balance</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  label\n",
       "49752                                     crisp framing       1\n",
       "24709                                       dislocation       0\n",
       "34945                     of the problems with the film       0\n",
       "28707  's ) a clever thriller with enough unexpected ...      1\n",
       "3013                                 in perfect balance       1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"****** First 5 rows of the validation   dataset are \\n\")\n",
    "validation_dataset.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bab20950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prior Probability of Positive Class in Training Set  is  0.5579 or 55.78999999999999\n",
      "Prior Probability of Negative Class in Training Set  is 0.4421 or 44.21\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 3: Calculating  the prior probability of each class in the training set\n",
    "positive_count = train_dataset['label'].sum()\n",
    "negative_count = len(train_dataset) - positive_count\n",
    "\n",
    "prior_probability_positive = positive_count / len(train_dataset)\n",
    "prior_probability_negative = negative_count / len(train_dataset)\n",
    "\n",
    "\n",
    "# Step 4: printing  the results\n",
    "\n",
    "print(f\"Prior Probability of Positive Class in Training Set  is  {prior_probability_positive.round(4)} or {prior_probability_positive.round(4)*100}\")\n",
    "print(f\"Prior Probability of Negative Class in Training Set  is {prior_probability_negative.round(4)} or {prior_probability_negative.round(4)*100}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af98412",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15a45a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647f46ae",
   "metadata": {},
   "source": [
    "# PROBLEM 2 – Tokenizing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6a941609",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loading  the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# A function for tokenizing \n",
    "def tokenizer(sentence):\n",
    "    # Tokenizing  the sentence using spaCy\n",
    "    tokens = [tok.text for tok in nlp(sentence)]\n",
    "    \n",
    "    # Adding  start and end symbols\n",
    "    tokens = ['<s>'] + tokens + ['</s>']\n",
    "    \n",
    "    return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb082808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bellow is a tockenized  of sentence\n",
      "['<s>', 'hide', 'new', 'secretions', 'from', 'the', 'parental', 'units', '</s>']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Applying the tokenizing  function to all sentences in the training set\n",
    "tokenized_sentences = [tokenizer(sentence) for sentence in train_dataset['sentence']]\n",
    "\n",
    "# Displaying  the tokenization of the first  sentence \n",
    "for i in range(1):  \n",
    "    print(f\"Bellow is a tockenized  of sentence\")\n",
    "    print(tokenized_sentences[i])\n",
    "\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6aaf50c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size including start and end symbols is 13882\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Collecting  all unique tokens from the training set\n",
    "unique_tokens = set()\n",
    "\n",
    "for sentence in train_dataset['sentence']:  \n",
    "    tokens = tokenizer(sentence)\n",
    "    unique_tokens.update(tokens)\n",
    "\n",
    "# Vocabulary size, including start and end symbols\n",
    "vocabulary_size = len(unique_tokens)\n",
    "\n",
    "print(f\"Vocabulary size including start and end symbols is {vocabulary_size}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08724889",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c73dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b69b28a",
   "metadata": {},
   "source": [
    "# PROBLEM 3 – Bigram counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9afda802",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def count_bigrams(tokenized_sequences):\n",
    "    bigram_counts = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    for sequence in tokenized_sequences:\n",
    "        for i in range(len(sequence) - 1):\n",
    "            wi, wj = sequence[i], sequence[i + 1]\n",
    "            bigram_counts[wi][wj] += 1\n",
    "    \n",
    "    return bigram_counts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3fdd14f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count *****\n",
      "\n",
      "Count of \"<s>\", \"the\" is  4426\n"
     ]
    }
   ],
   "source": [
    "# Applying  the function to the tokenized sentences from problem 2\n",
    "bigram_counts = count_bigrams(tokenized_sentences)\n",
    "\n",
    "# To find the count of \"<s>\", \"the\"\n",
    "start_the_count = bigram_counts[\"<s>\"][\"the\"]\n",
    "\n",
    "# Displaying  the count of \"<s>\", \"the\"\n",
    "print(\"Count *****\\n\")\n",
    "print(f'Count of \"<s>\", \"the\" is  {start_the_count}')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643d5a2a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# PROBLEM 4 – Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "37098d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def smoothing_function(wm, wm_1, bigram_counts, alpha, vocabulary_size):\n",
    "    \n",
    "    # Calculating  the count of the bigram (wm_1, wm)\n",
    "    bigram_count = bigram_counts.get(wm_1, {}).get(wm, 0)\n",
    "    \n",
    "    # Calculating  the total count of unigrams following wm_1\n",
    "    \n",
    "    total_count_wm_1 = sum(bigram_counts.get(wm_1, {}).values())\n",
    "    \n",
    "    # Applying  Laplace (add-one) smoothing to calculate the probability\n",
    "    \n",
    "    prob = (bigram_count + alpha) / (total_count_wm_1 + alpha * vocabulary_size)\n",
    "    \n",
    "    # Calculating  the negative log-probability\n",
    "    log_prob = -math.log(prob)\n",
    "    \n",
    "    return log_prob\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "371b1f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculating  the log probability for \"academy\" followed by \"award\" with alpha=0.001\n",
    "\n",
    "log_prob_alpha_0_001 = smoothing_function(\"academy\",\"award\", bigram_counts, vocabulary_size, 0.001)\n",
    "\n",
    "# Calculating  the log probability for \"academy\" followed by \"award\" with alpha=0.5\n",
    "log_prob_alpha_0_5 = smoothing_function(\"academy\",\"award\", bigram_counts, vocabulary_size,0.5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "62b7ec25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Probability \"academy\" , \"award\" with alpha=0.001 is -5.5331628899972465\n",
      "\n",
      "Log Probability \"academy\" , \"award\" with alpha=0.5 is  -0.6872576282345477 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# printing  the results\n",
    "print(f'Log Probability \"academy\" , \"award\" with alpha=0.001 is {log_prob_alpha_0_001}\\n')\n",
    "print(f'Log Probability \"academy\" , \"award\" with alpha=0.5 is  {log_prob_alpha_0_5} \\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8217c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b8f3ff",
   "metadata": {},
   "source": [
    "# PROBLEM 5 – Sentence log-probability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "176d0d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sentence_log_probability(sentence, bigram_counts, alpha, vocabulary_size):\n",
    "    # Tokenizing  the sentence\n",
    "    tokens = sentence.split()\n",
    "    \n",
    "    # Initializing  the log probability\n",
    "    log_prob = 0.0\n",
    "    \n",
    "    # Calculating  the log probability for each bigram in the sentence\n",
    "    \n",
    "    for i in range(1, len(tokens)):\n",
    "        wi, wm_1 = tokens[i], tokens[i - 1]\n",
    "        log_prob += smoothing_function(wi, wm_1, bigram_counts, alpha, vocabulary_size)\n",
    "    \n",
    "    return log_prob\n",
    "\n",
    "# Using the bellow  sentences as examples\n",
    "sentence1 = \"this was a really great movie but it was a little too long.\"\n",
    "sentence2 = \"long too little a was it but movie great really a was this.\"\n",
    "\n",
    "\n",
    "# Calculating  log probability for each  sentence\n",
    "log_prob1 = sentence_log_probability(sentence1, bigram_counts, vocabulary_size,0.001)\n",
    "log_prob2 = sentence_log_probability(sentence2, bigram_counts, vocabulary_size,0.001)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bb76421e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Probability for Sentence 1 is  -21.656772595585622 \n",
      "\n",
      "Log Probability for Sentence 2 is -23.47224806279462\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# printing  the results\n",
    "print(f'Log Probability for Sentence 1 is  {log_prob1} \\n')\n",
    "print(f'Log Probability for Sentence 2 is {log_prob2}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bca3f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1962cb12",
   "metadata": {},
   "source": [
    "# PROBLEM 6 – Tuning Alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44b6c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "30738177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of alpha values \n",
    "alpha_values = [0.001, 0.01, 0.1]\n",
    "\n",
    "# Initializing  variables to store the best alpha and best log likelihood\n",
    "best_alpha = None\n",
    "best_log_likelihood = float('-inf')  # Initializing  with negative infinity\n",
    "\n",
    "# Iterating  over the alpha values and calculating  log likelihood for each\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    total_log_likelihood = 0.0  # Initializing  the total log likelihood for this alpha\n",
    "    \n",
    "    # loopig through the dataset sentenses\n",
    "    for sentence in validation_dataset['sentence']:  \n",
    "        total_log_likelihood += sentence_log_probability(sentence, bigram_counts, alpha, vocabulary_size)\n",
    "    \n",
    "    # Checking if this alpha has a better log likelihood\n",
    "    \n",
    "    if total_log_likelihood > best_log_likelihood:\n",
    "        best_log_likelihood = total_log_likelihood\n",
    "        best_alpha = alpha\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fea890f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log likelihood for alpha=0.001: 3844.9122163187403\n",
      "\n",
      "Log likelihood for alpha=0.01: 4215.431147046351\n",
      "\n",
      "Log likelihood for alpha=0.1: 5026.045015426829\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# printing  the log likelihood for each alpha value\n",
    "for alpha in alpha_values:\n",
    "    total_log_likelihood = 0.0\n",
    "    \n",
    "    for sentence in validation_dataset['sentence']:  \n",
    "        total_log_likelihood += sentence_log_probability(sentence, bigram_counts, alpha, vocabulary_size)\n",
    "    \n",
    "    print(f'Log likelihood for alpha={alpha}: {total_log_likelihood}\\n')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "23499f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha 0.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Displaying  the best alpha\n",
    "print(f'Best alpha {best_alpha}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c5c3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ec8bae",
   "metadata": {},
   "source": [
    "# PROBLEM 7 – Applying Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c84e6e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Separating  the training dataset into positive and negative sentences\n",
    "positive_training = train_dataset.loc[train_dataset['label'] == 1]\n",
    "negative_training = train_dataset.loc[train_dataset['label'] == 0]\n",
    "\n",
    "# Computing  vocabulary size and bigram counts for both datasets\n",
    "vocabulary_size_positive = len(set(word for sentence in positive_training['sentence'] for word in tokenizer(sentence)))\n",
    "vocabulary_size_negative = len(set(word for sentence in negative_training['sentence'] for word in tokenizer(sentence)))\n",
    "\n",
    "bigram_counts_positive = count_bigrams([tokenizer(sentence) for sentence in positive_training['sentence']])\n",
    "bigram_counts_negative = count_bigrams([tokenizer(sentence) for sentence in negative_training['sentence']])\n",
    "\n",
    "# Initializing  variables to keep track of predictions\n",
    "predicted_labels = []\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2cc4d800",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Prior probabilities as computed in Problem 1\n",
    "prior_probability_positive = positive_training.shape[0] / train_dataset.shape[0]\n",
    "prior_probability_negative = negative_training.shape[0] / train_dataset.shape[0]\n",
    "\n",
    "# For each sentence in the test set\n",
    "for sentence in test_set['sentence']:\n",
    "    # Computing  log probabilities for positive and negative sentiments\n",
    "    log_prob_positive = sentence_log_probability(sentence, bigram_counts_positive, selected_alpha, vocabulary_size_positive) + math.log(prior_probability_positive)\n",
    "    log_prob_negative = sentence_log_probability(sentence, bigram_counts_negative, selected_alpha, vocabulary_size_negative) + math.log(prior_probability_negative)\n",
    "    \n",
    "    # Assigning  the sentiment label based on the comparison of scores\n",
    "    if log_prob_positive > log_prob_negative:\n",
    "        predicted_labels.append(1)  # Positive sentiment\n",
    "    else:\n",
    "        predicted_labels.append(0)  # Negative sentiment\n",
    "\n",
    "# Calculating  the class distribution of predicted labels\n",
    "predicted_positive_count = predicted_labels.count(1)\n",
    "predicted_negative_count = predicted_labels.count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "af9b95df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* Class Distribution of Predicted Labels *********\n",
      "\n",
      "Predicted Positive Sentiment =  46\n",
      "\n",
      "Predicted Negative Sentiment = 54\n",
      "\n",
      "Accuracy = 0.14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Comparing  predicted labels to true sentiment labels in the test set\n",
    "true_labels = test_dataset['label'].tolist()\n",
    "\n",
    "# Calculating  the accuracy of the experiment\n",
    "correct_predictions = sum(1 for true, predicted in zip(true_labels, predicted_labels) if true == predicted)\n",
    "accuracy = correct_predictions / len(test_dataset)\n",
    "\n",
    "# printing  the class distribution and accuracy\n",
    "print('******* Class Distribution of Predicted Labels *********\\n')\n",
    "print(f'Predicted Positive Sentiment =  {predicted_positive_count}\\n')\n",
    "print(f'Predicted Negative Sentiment = {predicted_negative_count}\\n')\n",
    "print(f'Accuracy = {accuracy}\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
